{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gro√üerBeleg2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJDUW4TX2N/9jcCRmi5JCR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rineuman/GB2020/blob/master/Gro%C3%9FerBeleg2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpX0S7Mip7kn",
        "colab_type": "text"
      },
      "source": [
        "INSTALL LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzkUOWAwkqvZ",
        "colab_type": "code",
        "outputId": "e338a59f-5186-4988-821a-bed74ad6c3e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ93DjwArpsZ",
        "colab_type": "code",
        "outputId": "f20b70c2-7254-47bd-cf67-8ce9f19f4ec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install q keras==2.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: q in /usr/local/lib/python3.6/dist-packages (2.6)\n",
            "Requirement already satisfied: keras==2.1 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbXZOTOOrW5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjczNeUln0YW",
        "colab_type": "code",
        "outputId": "cdaef82f-226d-430f-ceb2-9ea425521833",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install imgaug\n",
        "!pip install Cython\n",
        "!pip install pycocotools\n",
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.6/dist-packages (0.2.9)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug) (4.1.2.30)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.7.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug) (3.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (0.16.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug) (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (2.8.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.4)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (1.1.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug) (4.4.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauP4U4kqK5a",
        "colab_type": "text"
      },
      "source": [
        "CLONE MATTERPORT REPOSITORY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8vM7mZqGQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/matterport/Mask_RCNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KlLr8Wdb_13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbFSv_YqqNg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "os.chdir('Mask_RCNN')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBn7gVtjbQhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/rineuman/barrierDataset.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsgW9H4SbHsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuaQ0Yr2q3NI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import skimage.draw\n",
        "import numpy as np\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"../\")\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
        "\n",
        "# Directory of images to run detection on\n",
        "IMAGE_DIR = os.path.join(ROOT_DIR, \"barrierDataset/val\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WEwSblUtME_",
        "colab_type": "text"
      },
      "source": [
        "CONFIGURATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPzlNYsRr3ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BarrierConfig(Config):\n",
        "    \"\"\"Configuration for training on the toy shapes dataset.\n",
        "    Derives from the base Config class and overrides values specific\n",
        "    to the toy shapes dataset.\n",
        "    \"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"barriers\"\n",
        "\n",
        "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
        "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 3  # background + 3 barriers\n",
        "\n",
        "    # Use small images for faster training. Set the limits of the small side\n",
        "    # the large side, and that determines the image shape.\n",
        "    # IMAGE_MIN_DIM = 128\n",
        "    # IMAGE_MAX_DIM = 128\n",
        "\n",
        "    # Use smaller anchors because our image and objects are small\n",
        "    # RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
        "\n",
        "    # Reduce training ROIs per image because the images are small and have\n",
        "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
        "    # TRAIN_ROIS_PER_IMAGE = 32\n",
        "\n",
        "    # Use a small epoch since the data is simple\n",
        "    STEPS_PER_EPOCH = 1\n",
        "\n",
        "    # use small validation steps since the epoch is small\n",
        "    # VALIDATION_STEPS = 5\n",
        "    \n",
        "config = BarrierConfig()\n",
        "config.display()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewQJZpEYuJl3",
        "colab_type": "text"
      },
      "source": [
        "DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAqjdUq6uLRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BarrierDataset(utils.Dataset):\n",
        "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
        "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
        "    The images are generated on the fly. No file access required.\n",
        "    \"\"\"\n",
        "\n",
        "    def load_barrier(self, dataset_dir, subset):\n",
        "        \"\"\"Generate the requested number of synthetic images.\n",
        "        count: number of images to generate.\n",
        "        height, width: the size of the generated images.\n",
        "        \"\"\"\n",
        "        # Add classes\n",
        "        self.add_class(\"barrier\", 1, \"door\")\n",
        "        self.add_class(\"barrier\", 2, \"stair\")\n",
        "        self.add_class(\"barrier\", 3, \"lift\")\n",
        "\n",
        "         # Train or validation dataset?\n",
        "        assert subset in [\"train\", \"val\"]\n",
        "        dataset_dir = os.path.join(dataset_dir, subset)\n",
        "\n",
        "        # Load annotations\n",
        "        # VGG Image Annotator saves each image in the form:\n",
        "        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n",
        "        #   'regions': {\n",
        "        #       '0': {\n",
        "        #           'region_attributes': {},\n",
        "        #           'shape_attributes': {\n",
        "        #               'all_points_x': [...],\n",
        "        #               'all_points_y': [...],\n",
        "        #               'name': 'polygon'}},\n",
        "        #       ... more regions ...\n",
        "        #   },\n",
        "        #   'size': 100202\n",
        "        # }\n",
        "        # We mostly care about the x and y coordinates of each region\n",
        "        annotations1 = json.load(open(os.path.join(dataset_dir, \"via_export_json.json\")))\n",
        "        # print(annotations1)\n",
        "        annotations = list(annotations1.values())  # don't need the dict keys\n",
        "\n",
        "        # The VIA tool saves images in the JSON even if they don't have any\n",
        "        # annotations. Skip unannotated images.\n",
        "        annotations = [a for a in annotations if a['regions']]\n",
        "\n",
        "        # Add images\n",
        "        for a in annotations:\n",
        "            print(a)\n",
        "            # Get the x, y coordinaets of points of the polygons that make up\n",
        "            # the outline of each object instance. There are stores in the\n",
        "            # shape_attributes (see json format above)\n",
        "            polygons = [r['shape_attributes'] for r in a['regions']]\n",
        "            barriers = [r['region_attributes']for r in a['regions']]\n",
        "            print(\"NextBarrier: \")\n",
        "            print(barriers)\n",
        "            print(len(barriers))\n",
        "            print(type(barriers[0]))\n",
        "\n",
        "\n",
        "            print(\"NextPolygon \")\n",
        "            print(polygons)\n",
        "            print(len(polygons))\n",
        "\n",
        "            doorPolygons = []\n",
        "            stairPolygons = []\n",
        "            liftPolygons = []\n",
        "            classIds = []\n",
        "\n",
        "            for i in range(len(barriers)):\n",
        "              currentBarrier = barriers[i].get(\"barrier\")\n",
        "              currentPolygon = polygons[i]\n",
        "              print(currentBarrier)\n",
        "              if currentBarrier == \"door\":\n",
        "                doorPolygons.append(currentPolygon)\n",
        "                classIds.append(1)\n",
        "              elif currentBarrier == \"stair\":\n",
        "                stairPolygons.append(currentPolygon)\n",
        "                classIds.append(2)\n",
        "              elif currentBarrier == \"lift\":\n",
        "                liftPolygons.append(currentPolygon)\n",
        "                classIds.append(3)\n",
        "              else:\n",
        "                print(\"ERROR NOT door, stair or lift\")\n",
        "            # load_mask() needs the image size to convert polygons to masks.\n",
        "            # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
        "            # the image. This is only managable since the dataset is tiny.\n",
        "            image_path = os.path.join(dataset_dir, a['filename'])\n",
        "            image = skimage.io.imread(image_path)\n",
        "            height, width = image.shape[:2]\n",
        "\n",
        "            self.add_image(\n",
        "                \"barrier\",  ## for a single class just add the name here\n",
        "                image_id=a['filename'],  # use file name as a unique image id\n",
        "                path=image_path,\n",
        "                width=width, height=height,\n",
        "                polygons=polygons,\n",
        "                class_ids=np.asarray(classIds))\n",
        "            \n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return the shapes data of the image.\"\"\"\n",
        "        print(\"Image Reference \")\n",
        "        print(self.image_info)\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"barrier\":\n",
        "            return info[\"path\"]\n",
        "        else:\n",
        "            super(self.__class__).image_reference(self, image_id)\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Generate instance masks for an image.\n",
        "       Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # If not a bottle dataset image, delegate to parent class.\n",
        "        image_info = self.image_info[image_id]\n",
        "        print(\"IMAGE INFO: \")\n",
        "        print(image_info)\n",
        "        #if image_info[\"source\"] != \"barrier\":\n",
        "        #    return super(self.__class__, self).load_mask(image_id)\n",
        "\n",
        "        # Convert polygons to a bitmap mask of shape\n",
        "        # [height, width, instance_count]\n",
        "        info = self.image_info[image_id]\n",
        "        print(type(info['class_ids']))\n",
        "\n",
        "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
        "                        dtype=np.int32)\n",
        "        for i, p in enumerate(info[\"polygons\"]):\n",
        "            # Get indexes of pixels inside the polygon and set them to 1\n",
        "            print(p['name'])\n",
        "            if p['name'] == 'polygon':\n",
        "              rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
        "            elif p['name'] == 'rect':\n",
        "              rr, cc = skimage.draw.polygon([p['y'], p['y'] + p['height']], [p['x'], p['x'] + p['width']])\n",
        "            elif p['name'] == 'circle':\n",
        "              rr, cc = skimage.draw.circle(p['cy'], p['cx'], p['r'])\n",
        "\n",
        "            mask[rr, cc, i] = i+1\n",
        "\n",
        "        # Return mask, and array of class IDs of each instance. Since we have\n",
        "        # one class ID only, we return an array of 1s\n",
        "        # Map class names to class IDs.\n",
        "        return mask.astype(np.bool), info['class_ids'].astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKFQ59i1NwfH",
        "colab_type": "text"
      },
      "source": [
        "Extra functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p-B82mVNqF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def color_splash(image, mask):\n",
        "    \"\"\"Apply color splash effect.\n",
        "    image: RGB image [height, width, 3]\n",
        "    mask: instance segmentation mask [height, width, instance count]\n",
        "    Returns result image.\n",
        "    \"\"\"\n",
        "    # Make a grayscale copy of the image. The grayscale copy still\n",
        "    # has 3 RGB channels, though.\n",
        "    gray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255\n",
        "    # We're treating all instances as one, so collapse the mask into one layer\n",
        "    mask = (np.sum(mask, -1, keepdims=True) >= 1)\n",
        "    # Copy color pixels from the original color image where mask is set\n",
        "    if mask.shape[0] > 0:\n",
        "        splash = np.where(mask, image, gray).astype(np.uint8)\n",
        "    else:\n",
        "        splash = gray\n",
        "    return splash\n",
        "\n",
        "\n",
        "def detect_and_color_splash(model, image_path=None, video_path=None):\n",
        "    assert image_path or video_path\n",
        "\n",
        "    # Image or video?\n",
        "    if image_path:\n",
        "        # Run model detection and generate the color splash effect\n",
        "        print(\"Running on {}\".format(args.image))\n",
        "        # Read image\n",
        "        image = skimage.io.imread(args.image)\n",
        "        # Detect objects\n",
        "        r = model.detect([image], verbose=1)[0]\n",
        "        # Color splash\n",
        "        splash = color_splash(image, r['masks'])\n",
        "        # Save output\n",
        "        file_name = \"splash_{:%Y%m%dT%H%M%S}.png\".format(datetime.datetime.now())\n",
        "        skimage.io.imsave(file_name, splash)\n",
        "    elif video_path:\n",
        "        import cv2\n",
        "        # Video capture\n",
        "        vcapture = cv2.VideoCapture(video_path)\n",
        "        width = int(vcapture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(vcapture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = vcapture.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        # Define codec and create video writer\n",
        "        file_name = \"splash_{:%Y%m%dT%H%M%S}.avi\".format(datetime.datetime.now())\n",
        "        vwriter = cv2.VideoWriter(file_name,\n",
        "                                  cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "                                  fps, (width, height))\n",
        "\n",
        "        count = 0\n",
        "        success = True\n",
        "        while success:\n",
        "            print(\"frame: \", count)\n",
        "            # Read next image\n",
        "            success, image = vcapture.read()\n",
        "            if success:\n",
        "                # OpenCV returns images as BGR, convert to RGB\n",
        "                image = image[..., ::-1]\n",
        "                # Detect objects\n",
        "                r = model.detect([image], verbose=0)[0]\n",
        "                # Color splash\n",
        "                splash = color_splash(image, r['masks'])\n",
        "                # RGB -> BGR to save image to video\n",
        "                splash = splash[..., ::-1]\n",
        "                # Add image to video writer\n",
        "                vwriter.write(splash)\n",
        "                count += 1\n",
        "        vwriter.release()\n",
        "    print(\"Saved to \", file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvtdDOAsNzn-",
        "colab_type": "text"
      },
      "source": [
        "Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G--k4iFvuRC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training dataset\n",
        "dataset_train = BarrierDataset()\n",
        "dataset_train.load_barrier(\"/content/Mask_RCNN/barrierDataset\", \"train\")\n",
        "dataset_train.prepare()\n",
        "\n",
        "# Validation dataset\n",
        "dataset_val = BarrierDataset()\n",
        "dataset_val.load_barrier(\"/content/Mask_RCNN/barrierDataset\", \"val\")\n",
        "dataset_val.prepare()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtgLf149uUzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load and display random samples\n",
        "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
        "for image_id in image_ids:\n",
        "    image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names, 20)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFg7A2bFuWYK",
        "colab_type": "text"
      },
      "source": [
        "CREATE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIf2AuVkuaat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model in training mode\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                          model_dir=MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4iVzvkjuccX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Which weights to start with?\n",
        "init_with = \"coco\"  # imagenet, coco, or last\n",
        "\n",
        "if init_with == \"imagenet\":\n",
        "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
        "elif init_with == \"coco\":\n",
        "    # Load weights trained on MS COCO, but skip layers that\n",
        "    # are different due to the different number of classes\n",
        "    # See README for instructions to download the COCO weights\n",
        "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
        "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
        "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "elif init_with == \"last\":\n",
        "    # Load the last model you trained and continue training\n",
        "    model.load_weights(model.find_last(), by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "715Nr9I3ufhx",
        "colab_type": "text"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv4eBr6auhYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head\n",
        "# layers. You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE, \n",
        "            epochs=1, \n",
        "            layers='heads')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmlJ9GQpuoSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fine tune all layers\n",
        "# Passing layers=\"all\" trains all layers. You can also \n",
        "# pass a regular expression to select which layers to\n",
        "# train by name pattern.\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE / 10,\n",
        "            epochs=2, \n",
        "            layers=\"all\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoEz2ghSurQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save weights\n",
        "# Typically not needed because callbacks save after every epoch\n",
        "# Uncomment to save manually\n",
        "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
        "# model.keras_model.save_weights(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7wNxS2tuuSu",
        "colab_type": "text"
      },
      "source": [
        "DETECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNZ0iAA9uxg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InferenceConfig(ShapesConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", \n",
        "                          config=inference_config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "model_path = model.find_last()\n",
        "\n",
        "# Load trained weights\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dtvp0jFuz47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test on a random image\n",
        "image_id = random.choice(dataset_val.image_ids)\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset_val, inference_config, \n",
        "                           image_id, use_mini_mask=False)\n",
        "\n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        "\n",
        "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
        "                            dataset_train.class_names, figsize=(8, 8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwDQ1Z-eu6Km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], ax=get_ax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-hBvq1ou7a9",
        "colab_type": "text"
      },
      "source": [
        "EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3011PS9Xu9Xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute VOC-Style mAP @ IoU=0.5\n",
        "# Running on 10 images. Increase for better accuracy.\n",
        "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
        "APs = []\n",
        "for image_id in image_ids:\n",
        "    # Load image and ground truth data\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_val, inference_config,\n",
        "                               image_id, use_mini_mask=False)\n",
        "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
        "    # Run object detection\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    # Compute AP\n",
        "    AP, precisions, recalls, overlaps =\\\n",
        "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "    APs.append(AP)\n",
        "    \n",
        "print(\"mAP: \", np.mean(APs))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}